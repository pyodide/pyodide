// AVX path (f32x8, 8 lanes)
// WebAssembly SIMD only supports 128-bit vectors.
// When compiling AVX intrinsics, Emscripten lowers each 256-bit op into two
// 128-bit ops. Here we duplicate the lower 128b so result = 2 Ã— SSE (shows
// upper half is active).
#include <immintrin.h>

#if !defined(__wasm_simd128__) || !defined(__AVX__)
#error "Requires -msimd128 and -mavx"
#endif

float
simd_avx_add8_sum(float a0,
                  float a1,
                  float a2,
                  float a3,
                  float b0,
                  float b1,
                  float b2,
                  float b3)
{
  __m256 va = _mm256_set_ps(a3, a2, a1, a0, a3, a2, a1, a0);
  __m256 vb = _mm256_set_ps(b3, b2, b1, b0, b3, b2, b1, b0);
  __m256 vc = _mm256_add_ps(va, vb);
  float out[8];
  _mm256_storeu_ps(out, vc);
  // Collapse 8-lane vector to scalar for JS/Python assertions
  float s = 0.0f;
  for (int i = 0; i < 8; i++) {
    s += out[i];
  }
  return s;
}

float
simd_avx_dot8(float a0,
              float a1,
              float a2,
              float a3,
              float b0,
              float b1,
              float b2,
              float b3)
{
  __m256 va = _mm256_set_ps(a3, a2, a1, a0, a3, a2, a1, a0);
  __m256 vb = _mm256_set_ps(b3, b2, b1, b0, b3, b2, b1, b0);
  __m256 vm = _mm256_mul_ps(va, vb);
  float out[8];
  _mm256_storeu_ps(out, vm);
  // Collapse 8-lane vector to scalar for JS/Python assertions
  float s = 0.0f;
  for (int i = 0; i < 8; i++) {
    s += out[i];
  }
  return s;
}
